% Diskussion und Fazit.tex

% Definiere Variablen
% \newcommand{\Messziel}{Messziel}
\newcommand{\InterpretationAllerErgebnisse}{
    Es ist eindeutig zu sehen, dass Mergesort in der tiefenbasierten Thread-Erzeugungsvariante am besten performt sowie dass Quicksort bei der Worker-Thread-Variante nach einem Work-Stealing-Ansatz die beste Laufzeit erzielt, exakt wie zu erwarten war.
    Es ist auch sehr eindeutig, dass diese Varianten nur leicht schlechtere Laufzeiten erreichen, als theoretisch möglich wäre.
    Ich hatte allerdings ebenfalls erwartet, dass die Ergebnisse aufgrund der anfangs genannten Overheads stärker von der theoretisch möglichen Laufzeit abweichen würden.
    Wenn man nun die entstehenden Overheads durch die Thread-Erzeugung mit den seriellen Laufzeiten vergleicht, erhält man einen guten Eindruck, ab wann Parallelisierung überhaupt einen Vorteil bietet.
    Um zum Beispiel 16 Threads effizient nutzen zu können, sollte eine Mindestlaufzeit von 1\,ms erreicht werden. Dies ist z.\,B. bei Quicksort der Fall, wenn ein \texttt{int}-Array die Mindestgröße von 20k hat.
    Zusammenfassend lässt sich festhalten, dass Mergesort insbesondere dann die bessere Wahl ist, wenn die \textit{Worst-Case}-Laufzeit von Quicksort nicht tolerierbar ist. Allerdings zeigt Quicksort für typische, nicht degenerierte Listen im Durchschnitt eine bessere Laufzeit, wobei die \textit{Best-} und \textit{Worst-Case}-Laufzeiten stets berücksichtigt werden sollten.
    Wenn man nun von beiden Algorithmen die jeweils beste Variante miteinander vergleicht, stellt man fest, dass Quicksort nach der Parallelisierung im Durchschnittsfall immer noch doppelt so schnell wie Mergesort ist.
}

%\section{Ausblick und weiterführende Überlegungen}
% Ausblick und weiterführende Überlegungen
\newcommand{\Ausblick}{
    Eine theoretische Variante könnte Mergesort als Grundalgorithmus verwenden und bei Teilarrays kleiner gleich 40k auf Quicksort wechseln. Dadurch lässt sich eine absehbare Worst-Case-Laufzeit gewährleisten, während die durchschnittliche Laufzeit gegenüber reinem Mergesort verbessert wird.

    Man kann ebenso die Worst-Case-Laufzeit von Quicksort unwahrscheinlicher machen. Ein Ansatz dafür ist, dass das Pivot-Element zufällig bestimmt wird.
    Man kann auch mehrere Elemente zufällig auswählen sowie die Listenränder und die Listenmitte in die Pivot-Auswahl einbeziehen und daraus den Median bilden.
    Die Anzahl der zufällig hinzugewählten Elemente könnte dabei abhängig von der Listengröße gewählt werden.
    Diese Methoden führen dazu, dass die Worst-Case-Laufzeit nur noch eine Frage der Wahrscheinlichkeit ist.
    Da die Wahrscheinlichkeit sinkt, je mehr Pivot-Elemente zufällig bestimmt werden, ist es in der Praxis dann extrem unwahrscheinlich, dass der Worst-Case jemals eintritt.

    Es gibt auch Varianten, die den Partitionierungs- und Merge-Schritt parallelisiert haben. Man kann diese Variante auch noch mit der rekursiv parallelisierten Variante kombinieren, um so immer von allen Threads profitieren zu können. Dies sorgt dann wieder für eine noch bessere Laufzeit.

    Ich möchte noch erwähnen, dass moderne CPUs auf einem Multi-Chipsatz-Design basieren.
    Bei Intel sind das alle CPUs mit Performance- und Effizienz-Kernen zugleich.
    Da diese Kerne unterschiedliche Taktraten und Leistungen erbringen können, erschwert dies die perfekte Parallelisierung.
    Bei AMD besteht zum Beispiel der 7950X3D aus einem 7800X3D und einem 7800X.
    Diese beiden Chips unterscheiden sich in der Cache-Größe, in der Cache-Latenz und in der Taktrate.
    Der 7800X3D überhitzt auch architekturbedingt leichter, da der 3D-Cache direkt auf den Kernen sitzt.
    Dies sorgt auch bei AMD dafür, dass sich diese CPU nur sehr schwer perfekt parallelisieren lässt.
    Aus diesen Gründen sowie aufgrund der anfangs genannten Overheads halte ich eine perfekte Parallelisierung moderner CPUs für unmöglich. Stattdessen kann man sich dem idealen Szenario nur annähern.
}

% \section{Beantwortung der Forschungsfrage}
% Forschungsfrage beantworten
\newcommand{\BeantwortungDerForschungsfrage}{
    Von den von mir programmierten Varianten besitzt ganz klar die Quicksort-Worker-Stealing-Variante die beste Laufzeit, unter der Bedingung, dass das zu sortierende \texttt{int}-Array mindestens 20k bzw. das \texttt{String}-Array mindestens 8k groß ist.
    Erst wenn die sehr unwahrscheinliche \textit{Worst-Case}-Laufzeit untolerierbar ist, stellt die tiefenbasierte Thread-Erzeugung von Mergesort die bessere Wahl dar.
    Diese bietet aber erst eine bessere Laufzeit, wenn das zu sortierende \texttt{int}-Array mindestens 8k bzw. das \texttt{String}-Array mindestens 4k groß ist.
    Wenn das zu sortierende Array jedoch kleiner ist, ist die serielle Variante zu bevorzugen, da diese in diesem Bereich schneller fertig ist. Dies ist auf die anfangs genannten Overheads zurückzuführen.
    Man sollte hierbei beachten, dass sich diese Angaben zur verbesserten parallelen Laufzeit bei beiden Algorithmen auf die jeweilige 16-Thread-Variante beziehen.
}

% \section{Zusammenfassung}
% Kurze Zusammenfassung der Arbeit
\newcommand{\Zusammenfassung}{
    Zusammenfassend läst sich sagen das die Quicksort-Worker-Stealing-Variante algeime der Mergsort tiefenbasierte Thread-Erzeugung varaine überlgen ist. Und man darafu achten erste von der serinen versio zu PArelen umzueschletn wen die enstpschde midst lautz, bzw midest aRay göße ereciht sit. Zudem habe ich gezeigt das man nur fast nicht die tehrische möglscuhe lauzet verbserung ereicht.
    Es hat sich ebesno rausgestel das kompler optiming ein erhblichn einfuss auf die lauzet hat, vor allen wen es um thads geht.
    (auch wen ich daruf nciht in dsier arbeit expizit drauf eingegagen bin.)
    Zu dem sit noch viel verbeungs potzenal nach oben wie ich bereits im Ausblick erlöutert habe.
}

% Aufgrund der weiterhin bestehenden Worst-Case-Komplexität von Quicksort ist für sehr große Arrays die Verwendung von Mergesort eindeutig zu bevorzugen.
% \newline
% Für typische, nicht degenerierte Listen zeigt Quicksort im Durchschnitt eine konstante und effiziente Leistungssteigerung, wobei die Best- und Worst-Case-Laufzeiten stets berücksichtigt werden sollten.
% Diskussion und Fazit.tex

% Definiere Variablen
% \newcommand{\Messziel}{Messziel}
\newcommand{\InterpretationAllerErgebnisse}{
    Es sit eindeutig zu shen das Mergsort in der tiefenbasierte Thread-Erzeugung Varainte am besten perfomt, sowie dass Quicksort bei der
    Worker-Thread-Variante nach einem Work-Stealing-Ansatz
    die beste leuzeit erziehtlt. exakt wie zu erwarten war.
    Es ist auch sher eindeutg das diese varainten nur leicht schlchter lauzeten erhilhen wie thrisch möglscih wäre.
    Ich härte aleding auch erawtet das man stärker von der thrisch möglscihen lauzet abswichne würde.
    Wenman jetzt die enteheden Overhads duch die trhad erzeugen mit den Sereilen lauzeten vergleciht erhält man ein guten eindruck ab wann paralsirung überahupt was bringen kan.
    Um zum beipeil 16 Thrads profitern zu könen, solte eine midsetlauzeit von 1ms ereicht werden. Dis ist z.b. bei qiksort der fall wen ein int Aray die midest göße von 20k hat.
    Zusammenfassend lässt sich festhalten, dass Mergesort insbesondere dann die bessere Wahl ist, wenn die Worst-Case-Laufzeit von Quicksort nicht tolerierbar ist. Alerdings für typische, nicht degenerierte Listen zeigt Quicksort im Durchschnitt eine konstante und effiziente Leistungssteigerung, wobei die Best- und Worst-Case-Laufzeiten stets berücksichtigt werden sollten.

}
%\section{Ausblick und weiterführende Überlegungen}
% Ausblick und weiterführende Überlegungen
\newcommand{\Ausblick}{
    Eine theoretische Variante könnte Mergesort als Grundalgorithmus verwenden und bei Teilarrays kleiner gleich 40k auf Quicksort wechseln. Dadurch lässt sich eine absehbare Worst-Case-Laufzeit gewährleisten, während die durchschnittliche Laufzeit gegenüber reinem Mergesort verbessert wird.

    Man kann ebenso die Worst-Case-Laufzeit von Quicksort unwahrscheinlicher machen. Ein Ansatz dafür ist, dass das Pivot-Element zufällig bestimmt wird.
    Man kann auch mehrere Elemente zufällig auswählen sowie die Listenränder und die Listenmitte in die Pivot-Auswahl einbeziehen und daraus den Median bilden.
    Die Anzahl der zufällig hinzugewählten Elemente könnte dabei abhängig von der Listengröße gewählt werden.
    Diese Methoden führen dazu, dass die Worst-Case-Laufzeit nur noch eine Frage der Wahrscheinlichkeit ist.
    Da die Wahrscheinlichkeit sinkt, je mehr Pivot-Elemente zufällig bestimmt werden, ist es in der Praxis dann extrem unwahrscheinlich, dass der Worst-Case jemals eintritt.

    Es gibt auch Varianten, die den Partitionierungs- und Merge-Schritt parallelisiert haben. Man kann diese Variante auch noch mit der rekursiv parallelisierten Variante kombinieren, um so immer von allen Threads profitieren zu können. Dies sorgt dann wieder für eine noch bessere Laufzeit.

    Ich möchte noch erwähnen, dass moderne CPUs auf einem Multi-Chipsatz-Design basieren.
    Bei Intel sind das alle CPUs mit Performance- und Effizienz-Kernen zugleich.
    Da diese Kerne unterschiedliche Taktraten und Leistungen erbringen können, erschwert dies die perfekte Parallelisierung.
    Bei AMD besteht zum Beispiel der 7950X3D aus einem 7800X3D und einem 7800X.
    Diese beiden Chips unterscheiden sich in der Cache-Größe, in der Cache-Latenz und in der Taktrate.
    Der 7800X3D überhitzt auch architekturbedingt leichter, da der 3D-Cache direkt auf den Kernen sitzt.
    Dies sorgt auch bei AMD dafür, dass sich diese CPU nur sehr schwer perfekt parallelisieren lässt.
    Aus diesen Gründen sowie aufgrund der anfangs genannten Overheads halte ich eine perfekte Parallelisierung moderner CPUs für unmöglich. Stattdessen kann man sich dem idealen Szenario nur annähern.
}

% Auf Paralel merge und PArtionire eingehen.

% Aufgrund der weiterhin bestehenden Worst-Case-Komplexität von Quicksort ist für sehr große Arrays die Verwendung von Mergesort eindeutig zu bevorzugen.
% \newline
% Für typische, nicht degenerierte Listen zeigt Quicksort im Durchschnitt eine konstante und effiziente Leistungssteigerung, wobei die Best- und Worst-Case-Laufzeiten stets berücksichtigt werden sollten.
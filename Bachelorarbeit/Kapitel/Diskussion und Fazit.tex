% Diskussion und Fazit.tex

% Definiere Variablen
% \newcommand{\Messziel}{Messziel}
\newcommand{\InterpretationAllerErgebnisse}{
    Es ist eindeutig zu sehen, dass Mergesort in der tiefenbasierten Thread-Erzeugungsvariante am besten performt sowie dass Quicksort bei der Worker-Thread-Variante nach einem Work-Stealing-Ansatz die beste Laufzeit erzielt, exakt wie zu erwarten war.
    Es ist auch sehr eindeutig, dass diese Varianten nur leicht schlechtere Laufzeiten erreichen, als theoretisch möglich wäre.
    Ich hatte allerdings ebenfalls erwartet, dass die Ergebnisse aufgrund der anfangs genannten Overheads stärker von der theoretisch möglichen Laufzeit abweichen würden.
    Wenn man nun die entstehenden Overheads durch die Thread-Erzeugung mit den seriellen Laufzeiten vergleicht, erhält man einen guten Eindruck, ab wann Parallelisierung überhaupt einen Vorteil bietet.
    Um zum Beispiel 16 Threads effizient nutzen zu können, sollte eine Mindestlaufzeit von 1\,ms erreicht werden. Dies ist z.\,B. bei Quicksort der Fall, wenn ein \texttt{int}-Array die Mindestgröße von 20k hat.
    Zusammenfassend lässt sich festhalten, dass Mergesort insbesondere dann die bessere Wahl ist, wenn die \textit{Worst-Case}-Laufzeit von Quicksort nicht tolerierbar ist. Allerdings zeigt Quicksort für typische, nicht degenerierte Listen im Durchschnitt eine bessere Laufzeit, wobei die \textit{Best-} und \textit{Worst-Case}-Laufzeiten stets berücksichtigt werden sollten.
}
%\section{Ausblick und weiterführende Überlegungen}
% Ausblick und weiterführende Überlegungen
\newcommand{\Ausblick}{
    Eine theoretische Variante könnte Mergesort als Grundalgorithmus verwenden und bei Teilarrays kleiner gleich 40k auf Quicksort wechseln. Dadurch lässt sich eine absehbare Worst-Case-Laufzeit gewährleisten, während die durchschnittliche Laufzeit gegenüber reinem Mergesort verbessert wird.

    Man kann ebenso die Worst-Case-Laufzeit von Quicksort unwahrscheinlicher machen. Ein Ansatz dafür ist, dass das Pivot-Element zufällig bestimmt wird.
    Man kann auch mehrere Elemente zufällig auswählen sowie die Listenränder und die Listenmitte in die Pivot-Auswahl einbeziehen und daraus den Median bilden.
    Die Anzahl der zufällig hinzugewählten Elemente könnte dabei abhängig von der Listengröße gewählt werden.
    Diese Methoden führen dazu, dass die Worst-Case-Laufzeit nur noch eine Frage der Wahrscheinlichkeit ist.
    Da die Wahrscheinlichkeit sinkt, je mehr Pivot-Elemente zufällig bestimmt werden, ist es in der Praxis dann extrem unwahrscheinlich, dass der Worst-Case jemals eintritt.

    Es gibt auch Varianten, die den Partitionierungs- und Merge-Schritt parallelisiert haben. Man kann diese Variante auch noch mit der rekursiv parallelisierten Variante kombinieren, um so immer von allen Threads profitieren zu können. Dies sorgt dann wieder für eine noch bessere Laufzeit.

    Ich möchte noch erwähnen, dass moderne CPUs auf einem Multi-Chipsatz-Design basieren.
    Bei Intel sind das alle CPUs mit Performance- und Effizienz-Kernen zugleich.
    Da diese Kerne unterschiedliche Taktraten und Leistungen erbringen können, erschwert dies die perfekte Parallelisierung.
    Bei AMD besteht zum Beispiel der 7950X3D aus einem 7800X3D und einem 7800X.
    Diese beiden Chips unterscheiden sich in der Cache-Größe, in der Cache-Latenz und in der Taktrate.
    Der 7800X3D überhitzt auch architekturbedingt leichter, da der 3D-Cache direkt auf den Kernen sitzt.
    Dies sorgt auch bei AMD dafür, dass sich diese CPU nur sehr schwer perfekt parallelisieren lässt.
    Aus diesen Gründen sowie aufgrund der anfangs genannten Overheads halte ich eine perfekte Parallelisierung moderner CPUs für unmöglich. Stattdessen kann man sich dem idealen Szenario nur annähern.
}

% Auf Paralel merge und PArtionire eingehen.

% Aufgrund der weiterhin bestehenden Worst-Case-Komplexität von Quicksort ist für sehr große Arrays die Verwendung von Mergesort eindeutig zu bevorzugen.
% \newline
% Für typische, nicht degenerierte Listen zeigt Quicksort im Durchschnitt eine konstante und effiziente Leistungssteigerung, wobei die Best- und Worst-Case-Laufzeiten stets berücksichtigt werden sollten.
% Theoretische Grundlagen.tex

% Definiere Variablen
% \newcommand{\Messziel}{Messziel}

% ----------------------
% 2. Theoretische Grundlagen
% ----------------------
% \section{Theoretische Grundlagen}
% \subsection{Sortieralgorithmen: Quicksort und Mergesort}
% % Theorie zu Quicksort und Merge Sort
% \subsection{Grundlagen der Parallelisierung}
% % Amdahl Law
% \subsection{Thread-Modelle, Overheads und Skalierungsgrenzen}
% % Thread-Erzeugung, Synchronisation, Grenzen der Parallelisierung

% \subsection{Sortieralgorithmen: Quicksort und Mergesort}
% % Theorie zu Quicksort und Merge Sort
\newcommand{\SortieralgorithmenQuicksortMergesort}{
    Sowohl \textbf{Quicksort} als auch \textbf{Mergesort} basieren auf dem \emph{Teile-und-Herrsche}-Prinzip und sind rekursive Sortieralgorithmen. Dabei wird das zu sortierende Array wiederholt in kleinere Teilprobleme zerlegt, die unabhängig voneinander verarbeitet werden.

    \subsubsection{Mergesort}
    Das Grundprinzip von \textbf{Mergesort} besteht darin, aus zwei bereits sortierten Teillisten eine neue sortierte Liste zu erzeugen. In dieser Arbeit wird die unsortierte Ausgangsliste rekursiv in zwei möglichst gleich große Hälften geteilt, bis jede Teilliste nur noch aus einem einzelnen Element besteht. Da eine Liste mit einem Element per Definition sortiert ist, beginnt anschließend der sogenannte \emph{Merge-Schritt}. In diesem Schritt werden jeweils zwei sortierte Teillisten zu einer neuen sortierten Liste zusammengeführt.
    \newline
    Hierfür werden beide Teillisten sequenziell durchlaufen und die Elemente verglichen, wodurch pro Merge-Schritt \(n\) Vergleiche sowie \(2n\) Lese- und Schreibzugriffe erforderlich sind.
    \newline
    Die Laufzeit von Mergesort lässt sich durch die Rekurrenzgleichung
    \[
        T(n) = 2 \cdot T(n/2) + n
    \]
    beschreiben, wobei der Term \(+n\) den Aufwand des Merge-Schritts repräsentiert. Daraus ergibt sich eine Gesamtlaufzeit von
    \[
        T(n) = n \cdot \log_2(n)
    \]
    bzw.\ in asymptotischer Notation \(O(n \log n)\).

    \subsubsection{Quicksort}
    \textbf{Quicksort} ist im Grundaufbau ähnlich strukturiert, unterscheidet sich jedoch wesentlich im Ablauf. Die Liste wird nicht zwingend in zwei gleich große Hälften geteilt. Stattdessen wird zunächst ein sogenanntes \emph{Pivot-Element} gewählt, anhand dessen die Liste in einen kleineren und einen größeren Teil partitioniert wird. Dieser Partitionierungsschritt erfolgt \emph{vor} den rekursiven Selbstaufrufen, weshalb sich Quicksort auch als iterative Variante formulieren lässt.
    Beim Partitionieren wird die Liste so umsortiert, dass alle Elemente, die kleiner als das Pivotelement sind, links davon stehen und alle Elemente, die größer sind, rechts davon stehen. Dabei werden die Elemente auf beiden Seiten entsprechend getauscht.
    \newline
    Die Laufzeit von Quicksort hängt stark von der Qualität der Partitionierung ab. Im \textbf{Worst-Case}, beispielsweise bei ungünstiger Pivot-Wahl, beträgt die serielle Laufzeit
    \[
        T(n) = \frac{1}{2} \cdot ( n^2 + n ),
    \]
    \[
        O(T(n)) = n^2.
    \]
    Im \textbf{Best-Case} sowie im \textbf{Average-Case} ergibt sich hingegen ebenfalls die Rekurrenzgleichung
    \[
        T(n) = 2 \cdot T(n/2) + n,
    \]
    woraus wiederum eine Laufzeit von \(O(n \log n)\) folgt. Damit ist Quicksort im Durchschnitt asymptotisch genauso effizient wie Mergesort, aber in der Praxis ist Quicksort oft doppelt so schnell wie Mergesort, doch dazu später mehr.

}

% \subsection{Grundlagen der Parallelisierung}
% % Amdahl Law
\newcommand{\GrundlagenDerParallelisierung}{
    Parallelisierung beschreibt die gleichzeitige Ausführung mehrerer Programmteile mit dem Ziel, die Gesamtlaufzeit einer Berechnung zu reduzieren. Dabei wird eine ursprünglich serielle Aufgabe in mehrere Teilaufgaben zerlegt, die parallel auf mehreren Recheneinheiten verarbeitet werden können.
    \newline
    Der maximal erreichbare Geschwindigkeitsgewinn durch Parallelisierung ist jedoch begrenzt. Nach dem Amdahlschen Gesetz hängt die theoretische Beschleunigung davon ab, welcher Anteil eines Programms parallelisiert werden kann. Serielle Programmanteile sowie zusätzlicher Verwaltungsaufwand, beispielsweise durch Thread-Erzeugung, Synchronisation und Kommunikation, begrenzen die Skalierbarkeit.
    \newline
    In der Praxis führt Parallelisierung daher nicht zwangsläufig zu einer linearen Beschleunigung, insbesondere bei steigender Anzahl von Threads.
    \newline
    Einfach ausgedrückt bedeutet dies, dass Code mit seriellen Abhängigkeiten auch bei mehreren Threads nicht schneller ausgeführt wird. Daher sollte nur der Teil des Codes parallelisiert werden, der keine solchen Abhängigkeiten enthält.
}

% \subsection{Thread-Modelle, Overheads und Skalierungsgrenzen}
% % Thread-Erzeugung, Synchronisation, Grenzen der Parallelisierung
\newcommand{\ThreadModelleOverheadsUndSkalierungsgrenzen}{
    Bei Thrahds gibt es in der prxis immer Overhads die dafür sorgen das man eine perfekte Zeitersparnis nicht ereichen kann.
    Dise Overhads sind init-/join-zeiten, dekusturktor zeiten. Dazu komt das die Hardware auch in der Prxis genzen hatt was wider zu Overhads führt. Dise Hardware grenzen sind die thrad swaping zeiten, die begrantze kern anzahl (bei mir 8 kerne bzw 16 virtullle kerne), sowie kan es zu mer cach mises kommen wen man mehr thrads nutzt, da diser öfter benutzt wird. Dies führt widerum das mer Daten von Ram gelesen werden müssen was widerum dazu führen kann das man eher am Datenraten limit des ramcontolrers ist anstatt am rechenlimt der cpu kerne. Jetzt mus man auch bedekenden das man ziwcehen echten kerenen und virtuellen keren unterscheiden muss da ein virtuleer kern nicht so gut mit parelisung scalt wie ein echter aufgrund der begrenzeten hardware recurecen.

    Dann gibt es auch noch andre Skalierungsgrenzen wie die Listen göße, da wen eine lsite mer speicher bracuht als der ganze ram biten kan disen speicher auslgern muss, und dier ausgelgerter speicher ist wesenlich langsamer im lesen schreben sowe in den latzenen dafür.

    Dann gibt es natülich auch verschidene methoden wie man Code paraliseren kann. die einfachste ist den nicht sereien code immer in ein neuen thrad asuzulagern. Dise ist jedoch je nach anwedungs fall nicht immer sinnfoll, da man sonst zu vilee thrads erstellt die sich gegeseitetig duch thrads swoping oder gar duch den erhäteh ram verbrauch von den thrads selber gegseitig verlagsomern.
    Daher begrent man in normal fall die Thrad anzal um sowas zu vermeiden, das ist denke ich für anfänger die beste startegie.
    Dann kann man das genze natülich noch verbessern, indem man versucht die Thrad initzeiten nur eieinzigen mal für den geamten code vraucht, dise satatie gie wird dann mitels worker Trahds umgestetzt. Dise erstellt man einmal am anfang es codes und weisten ihnen imer wider neue aufgaben zu anstett wen sie fertig sind eichaf wider zu zerstören. Dann kan man das ganzie imer noch verändern in dem man die art und weise wie man diese workerrthads auszutz ändert, damit miene ich das man dise nur Aufgen zuwasien bracuht wen sie frei sind und anstat zu warten das sie frei werden im eigenen thrad direkt dise aufgbe zu erledigen. Dise startegie hat je nach anwedungsfall vor und nachteile, diese werde ich später mit messungen zeigen.
}
% Theoretische Grundlagen.tex

% Definiere Variablen
% \newcommand{\Messziel}{Messziel}

% ----------------------
% 2. Theoretische Grundlagen
% ----------------------
% \section{Theoretische Grundlagen}
% \subsection{Sortieralgorithmen: Quicksort und Mergesort}
% % Theorie zu Quicksort und Merge Sort
% \subsection{Grundlagen der Parallelisierung}
% % Amdahl Law
% \subsection{Thread-Modelle, Overheads und Skalierungsgrenzen}
% % Thread-Erzeugung, Synchronisation, Grenzen der Parallelisierung

\newcommand{\HinweisFormelanpassung}{
    \paragraph{Hinweis zur mathematischen Darstellung}
    Die in dieser Arbeit genutzten mathematischen Beschreibungen und Formeln beziehen sich durchgehend auf die konkret implementierten Programmstrukturen und können daher von allgemeinen Standardformeln abweichen.
}

% \subsection{Sortieralgorithmen: Quicksort und Mergesort}
% % Theorie zu Quicksort und Merge Sort
\newcommand{\SortieralgorithmenQuicksortMergesort}{
    Sowohl \textbf{Quicksort} als auch \textbf{Mergesort} basieren auf dem \emph{Teile-und-Herrsche}-Prinzip und sind rekursive Sortieralgorithmen. Dabei wird das zu sortierende Array wiederholt in kleinere Teilprobleme zerlegt, die unabhängig voneinander verarbeitet werden.

    \subsubsection{Mergesort}
    Das Grundprinzip von \textbf{Mergesort} besteht darin, zwei bereits sortierte Teilarrays zu einem sortierten Array zusammenzuführen. In dieser Arbeit wird das unsortierte Eingabearray rekursiv in zwei möglichst gleich große Hälften geteilt, bis jedes Teilarray nur noch aus einem einzelnen Element besteht. Da ein Array mit einem Element per Definition sortiert ist, beginnt anschließend der sogenannte \emph{Merge-Schritt}. In diesem Schritt werden jeweils zwei sortierte Teilarrays zu einem sortierten Gesamtergebnis zusammengeführt.
    \newline
    Hierfür werden beide Teilarrays mit einer Gesamtlänge von \(n\) Elementen sequenziell durchlaufen und die Elemente verglichen.
    Der Aufwand pro Merge-Schritt entspricht dabei \(n\) Vergleichen, da jedes Element genau einmal betrachtet wird, sowie \(2n\) Lese- und Schreibzugriffen, da die Elemente temporär in ein neues Array der Größe \(n\) geschrieben und von dort wieder gelesen werden müssen.
    \newline
    Die Laufzeit von Mergesort lässt sich durch die Rekursionsgleichung
    \[
        T(n) = 2 \cdot T \left( \frac{n}{2} \right) + n
    \]
    beschreiben, wobei der Term \( {} + n \) den Aufwand des Merge-Schritts repräsentiert
    und \( 2 \cdot T \left( \frac{n}{2} \right) \) die beiden rekursiven Selbstaufrufe darstellt.
    Daraus ergibt sich eine Gesamtlaufzeit von
    \[
        T(n) = n \cdot \log_2(n) + n
    \]
    bzw. in asymptotischer Notation \(O(n \log n)\).

    \subsubsection{Quicksort}
    \textbf{Quicksort} ist im Grundaufbau ähnlich strukturiert, unterscheidet sich jedoch wesentlich im Ablauf. Die Liste wird nicht zwingend in zwei gleich große Hälften geteilt. Stattdessen wird zunächst ein sogenanntes \emph{Pivot-Element} gewählt, anhand dessen die Liste in einen kleineren und einen größeren Teil partitioniert wird. Dieser Partitionierungsschritt erfolgt vor den rekursiven Selbstaufrufen.
    Beim Partitionieren wird die Liste so umsortiert, dass alle Elemente, die kleiner als das Pivotelement sind, links davon stehen und alle Elemente, die größer sind, rechts davon stehen. Dabei werden die Elemente auf beiden Seiten entsprechend getauscht.
    \newline
    Die Laufzeit von Quicksort hängt stark von der Qualität der Partitionierung ab.
    Der \textbf{heuristisch betrachtete Average-Case} ergibt sich aus den rekursiven Selbstaufrufen.
    Die Herleitung sowie die Endgleichung für die serielle Laufzeit lauten:
    \begin{flalign*}
        T(n)    & = q_1 + q_2 + n                                                                                                                               & \\
        q_1     & = T\left( \frac{1 + \dots + n-1}{n-1} \right) = T\left( n \cdot \frac{n-1}{2} \cdot \frac{1}{n-1} \right) = T\left( \frac{n}{2} \right) = q_2 & \\
        T(n)    & = 2 \cdot T\left( \frac{n}{2} \right) + n                                                                                                     & \\
        T(n)    & = n \cdot \log_2(n) + n                                                                                                                       & \\
        O(T(n)) & = O(n \log n).                                                                                                                                &
    \end{flalign*}
    Der \textbf{Worst-Case} tritt ein, wenn das gewählte Pivotelement in jedem Rekursionsschritt zu einer maximal asymmetrischen Partitionierung führt.
    Für die serielle Laufzeit gilt hierbei:
    \begin{flalign*}
        T(n)    & = T(n-1) + 1 + n,                    & \\
        T(n)    & = \frac{1}{2} \cdot ( n^2 + n ) + n, & \\
        O(T(n)) & = O(n^2).                            &
    \end{flalign*}
    Der \textbf{Best-Case} tritt ein, wenn das gewählte Pivotelement in jedem Rekursionsschritt zu einer exakt symmetrischen Partitionierung führt.
    Für die serielle Laufzeit ergeben sich hierbei die Gleichungen:
    \begin{flalign*}
        T(n)    & = 2 \cdot T\left( \frac{n}{2} \right) + n & \\
        T(n)    & = n \cdot \log_2(n) + n                   & \\
        O(T(n)) & = O(n \log n).
    \end{flalign*}
    Aufgrund der theoretischen Best-Case- und heuristischen Average-Case-Laufzeit ist Quicksort im Durchschnitt genauso effizient wie Mergesort,
    aber in der Praxis ist Quicksort oft doppelt so schnell wie Mergesort, doch dazu später mehr.
}

% \subsection{Grundlagen der Parallelisierung}
% % Amdahl Law
\newcommand{\GrundlagenDerParallelisierung}{
    Parallelisierung beschreibt die gleichzeitige Ausführung mehrerer Programmteile mit dem Ziel, die Gesamtlaufzeit einer Berechnung zu reduzieren. Dabei wird eine ursprünglich serielle Aufgabe in mehrere Teilaufgaben zerlegt, die parallel auf mehreren Recheneinheiten verarbeitet werden können.
    \newline
    Der maximal erreichbare Geschwindigkeitsgewinn durch Parallelisierung ist jedoch begrenzt. Nach dem Amdahlschen Gesetz hängt die theoretische Beschleunigung davon ab, welcher Anteil eines Programms parallelisiert werden kann. Serielle Programmanteile sowie zusätzlicher Verwaltungsaufwand, beispielsweise durch Thread-Erzeugung, Synchronisation und Kommunikation, begrenzen die Skalierbarkeit.
    \newline
    In der Praxis führt Parallelisierung daher nicht zwangsläufig zu einer linearen Beschleunigung, insbesondere bei steigender Anzahl von Threads.
    \newline
    Einfach ausgedrückt bedeutet dies, dass Code mit seriellen Abhängigkeiten auch bei mehreren Threads nicht schneller ausgeführt wird. Daher sollte nur der Teil des Codes parallelisiert werden, der keine solchen Abhängigkeiten enthält.
}

% \subsection{Thread-Modelle, Overheads und Skalierungsgrenzen}
% % Thread-Erzeugung, Synchronisation, Grenzen der Parallelisierung
\newcommand{\ThreadModelleOverheadsUndSkalierungsgrenzen}{
    In der Praxis verursachen Threads verschiedene Overheads, die verhindern, dass eine theoretisch ideale Zeitersparnis (linearer Speedup) erreicht wird. Zu diesen Overheads zählen primär die Initialisierungs- und Join-Zeiten, die Laufzeiten von Destruktoren sowie die notwendige Synchronisation bei Abhängigkeiten zwischen Threads. Um die Datenkonsistenz zu gewährleisten, müssen Mechanismen wie Sperren (Mutexe) oder Barrieren (Synchronisationspunkte) eingesetzt werden, welche zusätzliche Wartezeiten und Verwaltungsoverheads verursachen. Parallel dazu setzen Hardware-Limitierungen der Skalierung Grenzen. Hierbei beeinflussen Context-Switching-Zeiten bei Überbelegung der Kerne (Oversubscription), die begrenzte Anzahl physischer Kerne (im Testsystem 8 physische bzw. 16 logische Prozessoren) sowie eine erhöhte Rate an Cache-Misses bei steigender Thread-Anzahl die Performance negativ. Letzteres führt dazu, dass vermehrt Daten aus dem RAM geladen werden müssen, wodurch das System je nach Anwendungsfall eher durch die Bandbreite des Speichercontrollers (Memory Bound) als durch die Rechenleistung der CPU-Kerne begrenzt wird. Zudem ist zwischen physischen und logischen Prozessoren zu unterscheiden, da letztere aufgrund geteilter Hardware-Ressourcen weniger effizient skalieren.
    \newline
    Eine weitere wesentliche Hardware-Grenze stellt die CPU dar. Moderne CPUs sind durch eine maximale Leistungsaufnahme (Thermal Design Power, TDP) begrenzt. Eine höhere Auslastung aller Kerne führt daher nicht zwangsläufig zu proportional höherer Leistung. Beispielsweise weist die genutzte CPU einen Single-Core-Boost-Takt von 4,75 GHz auf, jedoch nur einen All-Core-Takt von 4,6 GHz, wodurch einzelne Threads bei geringer Auslastung performanter laufen. Zudem wird ein Großteil der TDP als Abwärme freigegeben, die effizient abgeführt werden muss. Bei unzureichender Kühlung oder Überschreitung thermischer Grenzen kommt es zur automatischen thermischen Drosselung (Thermal Throttling) der CPU, wodurch der Takt des jeweiligen Kerns temporär reduziert wird. Diese Faktoren beeinflussen die Performance ebenfalls negativ.
    \newline
    Eine weitere Skalierungsgrenze stellt die Datensatzgröße dar. Übersteigt der Speicherbedarf die Kapazität des RAMs, muss das Betriebssystem Teile des Speichers auslagern (Page-Out). Da sowohl die Zugriffslatenzen als auch die Datenübertragungsraten von Sekundärspeichern (wie SSDs) signifikant schlechter sind als die des Arbeitsspeichers, führt dies zu massiven Performance-Einbußen.
    \newline
    Hinsichtlich der Implementierung existieren verschiedene Ansätze. Die simpelste Methode besteht darin, jeden nicht-sequentiellen Codeabschnitt in einen neuen Thread auszulagern. Dies ist jedoch oft kontraproduktiv, da ein Übermaß an Threads zu Performance-Verlusten durch Context Switching und hohen Speicherverbrauch führt. In der Praxis wird die Thread-Anzahl daher meist limitiert.
    \newline
    Eine Optimierung stellt die Nutzung von Worker-Threads dar. Hierbei werden Threads einmalig initialisiert und verbleiben über die gesamte Laufzeit aktiv, um kontinuierlich neue Aufgaben abzuarbeiten, anstatt nach jeder Aufgabe zerstört zu werden. Eine weiterführende Strategie ist das Dynamic Scheduling (oder Work-Stealing-Ansätze), bei dem Aufgaben nur dann zugewiesen werden, wenn Ressourcen frei sind. Sind alle Worker-Threads belegt, kann der aufrufende Thread die Aufgabe selbst bearbeiten, um Wartezeiten zu minimieren. Die Vor- und Nachteile dieser Strategien werden im Kapitel der Messungen detailliert analysiert.
}